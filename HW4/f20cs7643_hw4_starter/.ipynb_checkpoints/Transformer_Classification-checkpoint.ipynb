{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Sentence Classification with Transformers\n",
    "In this exercise you will implement a [Transformer](https://arxiv.org/pdf/1706.03762.pdf) and use it to judge the grammaticality of English sentences.\n",
    "\n",
    "**A quick note: if you receive the following TypeError \"super(type, obj): obj must be an instance or subtype of type\", try restarting your kernel and re-running all cells.** Once you have finished making changes to the model constuctor, you can avoid this issue by commenting out all of the model instantiations after the first (e.g. lines starting with \"model = ClassificationTransformer\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import torch\n",
    "\n",
    "from gt_7643.transformer import ClassificationTransformer\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Corpus of Linguistic Acceptability (CoLA)\n",
    "\n",
    "The Corpus of Linguistic Acceptability ([CoLA](https://nyu-mll.github.io/CoLA/)) in its full form consists of 10657 sentences from 23 linguistics publications, expertly annotated for acceptability (grammaticality) by their original authors. Native English speakers consistently report a sharp contrast in acceptability between pairs of sentences. \n",
    "Some examples include:\n",
    "\n",
    "`What did Betsy paint a picture of?` (Correct)\n",
    "\n",
    "`What was a picture of painted by Betsy?` (Incorrect)\n",
    "\n",
    "You can read more info about the dataset [here](https://arxiv.org/pdf/1805.12471.pdf). This is a binary classification task (predict 1 for correct grammar and 0 otherwise).\n",
    "\n",
    "Can we train a neural network to accurately predict these human acceptability judgements? In this assignment, we will implement the forward pass of the Transformer architecture discussed in class. The general intuitive notion is that we will _encode_ the sequence of tokens in the sentence, and then predict a binary output based on the final state that is the output of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the preprocessed data\n",
    "\n",
    "We've appended a \"CLS\" token to the beginning of each sequence, which can be used to make predictions. The benefit of appending this token to the beginning of the sequence (rather than the end) is that we can extract it quite easily (we don't need to remove paddings and figure out the length of each individual sequence in the batch). We'll come back to this.\n",
    "\n",
    "We've additionally already constructed a vocabulary and converted all of the strings of tokens into integers which can be used for vocabulary lookup for you. Feel free to explore the data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 1542\n",
      "(7000, 43)\n",
      "(1551, 43)\n",
      "(7000,)\n",
      "(1551,)\n"
     ]
    }
   ],
   "source": [
    "train_inxs = np.load('./gt_7643/datasets/train_inxs.npy')\n",
    "val_inxs = np.load('./gt_7643/datasets/val_inxs.npy')\n",
    "train_labels = np.load('./gt_7643/datasets/train_labels.npy')\n",
    "val_labels = np.load('./gt_7643/datasets/val_labels.npy')\n",
    "\n",
    "# load dictionary\n",
    "word_to_ix = {}\n",
    "with open(\"./gt_7643/datasets/word_to_ix.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for line in reader:\n",
    "        word_to_ix[line[0]] = line[1]\n",
    "print(\"Vocabulary Size:\", len(word_to_ix))\n",
    "        \n",
    "print(train_inxs.shape) # 7000 training instances, of (maximum/padded) length 43 words.\n",
    "print(val_inxs.shape) # 1551 validation instances, of (maximum/padded) length 43 words.\n",
    "print(train_labels.shape)\n",
    "print(val_labels.shape)\n",
    "\n",
    "# load checkers\n",
    "d1 = torch.load('./gt_7643/datasets/d1.pt')\n",
    "d2 = torch.load('./gt_7643/datasets/d2.pt')\n",
    "d3 = torch.load('./gt_7643/datasets/d3.pt')\n",
    "d4 = torch.load('./gt_7643/datasets/d4.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "We will be implementing a one-layer Transformer **encoder** which, similar to an RNN, can encode a sequence of inputs and produce a final output state for classification. This is the architecture:\n",
    "\n",
    "![imgs/encoder.png](imgs/encoder.png)\n",
    "\n",
    "\n",
    "You can refer to the [original paper](https://arxiv.org/pdf/1706.03762.pdf) for more details.\n",
    "\n",
    "Instead of using numpy for this model, we will be using Pytorch to implement the forward pass. You will not need to implement the backward pass for the various layers in this assigment.\n",
    "\n",
    "The file `gt_7643/transformer.py` contains the model class and methods for each layer. This is where you will write your implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverable 1: Embeddings\n",
    "\n",
    "We will format our input embeddings similarly to how they are constructed in [BERT (source of figure)](https://arxiv.org/pdf/1810.04805.pdf). Recall from lecture that unlike a RNN, a Transformer does not include any positional information about the order in which the words in the sentence occur. Because of this, we need to append a positional encoding token at each position. (We will ignore the segment embeddings and [SEP] token here, since we are only encoding one sentence at a time). We have already appended the [CLS] token for you in the previous step.\n",
    "![imgs/embedding.png](imgs/embedding.png)\n",
    "\n",
    "Your first task is to implement the embedding lookup, including the addition of positional encodings. Open the file `gt_7643/transformer.py` and complete all code parts for `Deliverable 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference: 0.0017988268518820405\n"
     ]
    }
   ],
   "source": [
    "inputs = train_inxs[0:2]\n",
    "inputs = torch.LongTensor(inputs)\n",
    "\n",
    "model = ClassificationTransformer(word_to_ix, hidden_dim=128, num_heads=2, dim_feedforward=2048, dim_k=96, \n",
    "                                  dim_v=96, dim_q=96, max_length=train_inxs.shape[1])\n",
    "\n",
    "embeds = model.embed(inputs)\n",
    "\n",
    "try:\n",
    "    print(\"Difference:\", torch.sum(torch.pairwise_distance(embeds, d1)).item()) # should be very small (<0.01)\n",
    "except:\n",
    "    print(\"NOT IMPLEMENTED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverable 2: Multi-head Self-Attention\n",
    "\n",
    "Attention can be computed in matrix-form using the following formula:\n",
    "![imgs/attn.png](imgs/attn.png)\n",
    "\n",
    "We want to have multiple self-attention operations, computed in parallel. Each of these is called a *head*. We concatenate the heads and multiply them with the matrix `attention_head_projection` to produce the output of this layer.\n",
    "\n",
    "After every multi-head self-attention and feedforward layer, there is a residual connection + layer normalization. Make sure to implement this, using the following formula: \n",
    "![imgs/layer_norm.png](imgs/layer_norm.png)\n",
    "\n",
    "\n",
    "Open the file `gt_7643/transformer.py` and implement the `multihead_attention` function. \n",
    "We have already initialized all of the layers you will need in the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference: 319.94830322265625\n"
     ]
    }
   ],
   "source": [
    "hidden_states = model.multi_head_attention(embeds)\n",
    "\n",
    "try:\n",
    "    print(\"Difference:\", torch.sum(torch.pairwise_distance(hidden_states, d2)).item()) # should be very small (<0.01)\n",
    "except:\n",
    "    print(\"NOT IMPLEMENTED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverable 3: Element-Wise Feed-forward Layer\n",
    "\n",
    "Open the file `gt_7643/transformer.py` and complete code for `Deliverable 3`: the element-wise feed-forward layer consisting of two linear transformers with a ReLU layer in between.\n",
    "\n",
    "![imgs/ffn.png](imgs/ffn.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference: 316.8460998535156\n"
     ]
    }
   ],
   "source": [
    "model = ClassificationTransformer(word_to_ix, hidden_dim=128, num_heads=2, dim_feedforward=2048, dim_k=96, \n",
    "                                  dim_v=96, dim_q=96, max_length=train_inxs.shape[1])\n",
    "\n",
    "outputs = model.feedforward_layer(hidden_states)\n",
    "\n",
    "try:\n",
    "    print(\"Difference:\", torch.sum(torch.pairwise_distance(outputs, d3)).item()) # should be very small (<0.01)\n",
    "except:\n",
    "    print(\"NOT IMPLEMENTED\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverable 4: Final Layer\n",
    "\n",
    "Open the file `gt_7643/transformer.py` and complete code for `Deliverable 4`, to produce binary classification scores for the inputs based on the output of the Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference: 0.03463253006339073\n"
     ]
    }
   ],
   "source": [
    "model = ClassificationTransformer(word_to_ix, hidden_dim=128, num_heads=2, dim_feedforward=2048, dim_k=96, \n",
    "                                  dim_v=96, dim_q=96, max_length=train_inxs.shape[1])\n",
    "\n",
    "scores = model.final_layer(outputs)\n",
    "\n",
    "try:\n",
    "    print(\"Difference:\", torch.sum(torch.pairwise_distance(scores, d4)).item()) # should be very small (<1e-5)\n",
    "except:\n",
    "    print(\"NOT IMPLEMENTED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverable 5: Putting it all together\n",
    "\n",
    "Open the file `gt_7643/transformer.py` and complete the method `forward`, by putting together all of the methods you have developed in the right order to perform a full forward pass.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference: 1.9999999949504854e-06\n"
     ]
    }
   ],
   "source": [
    "inputs = train_inxs[0:2]\n",
    "inputs = torch.LongTensor(inputs)\n",
    "\n",
    "outputs = model.forward(inputs)\n",
    "\n",
    "try:\n",
    "    print(\"Difference:\", torch.sum(torch.pairwise_distance(outputs, scores)).item()) # should be very small (<1e-5)\n",
    "except:\n",
    "    print(\"NOT IMPLEMENTED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We've just implemented a Transformer forward pass for text classification. One of the big perks of using PyTorch is that with a simple training loop, we can rely on automatic differentation ([autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)) to do the work of the backward pass for us. This is not required for this assignment, but you can explore this on your own.\n",
    "\n",
    "Make sure when you submit your PDF for this assignment to also include a copy of `transformer.py` converted to PDF as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "widgets": {
   "state": {},
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
